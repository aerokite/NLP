{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"executionInfo":{"elapsed":724,"status":"ok","timestamp":1643769598770,"user":{"displayName":"Mir Shahriar Sabuj","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgnIF9MHarEcGwKaxaBrV3ZiuS6e9hwUsKhsKJ_=s64","userId":"07212414285941174156"},"user_tz":-360},"id":"eF2dGu_GUCm0"},"outputs":[],"source":["\n","import torch\n","import numpy as np\n","from torchtext.data.metrics import bleu_score\n","\n","\n","def get_padding_mask(query, key, pad_idx):\n","    batch_size, len_query = query.size()\n","    batch_size, len_key = key.size()\n","    masking = key.data.eq(pad_idx).unsqueeze(1)\n","    return masking.expand(batch_size, len_query, len_key)\n","\n","def get_subsequent_mask(query, device):\n","    shape = [query.size(0), query.size(1), query.size(1)]\n","    subsequent_mask = np.tril(np.ones(shape), k=0) == 0\n","    subsequent_mask = torch.from_numpy(subsequent_mask).to(device)\n","    return subsequent_mask\n","\n","\n","# This method translate a sentence to target language\n","def translate_sentence(model, sentence, src_spacy_model, source_field, targer_field, device, max_length=60):\n","    \n","    if type(sentence) == str:\n","        input_tokens = [token.text.lower() for token in src_spacy_model(sentence)]\n","    else:\n","        input_tokens = [token.lower() for token in sentence]\n","\n","\n","    # Add <sos> and <eos>\n","    input_tokens.insert(0, source_field.init_token)\n","    input_tokens.append(source_field.eos_token)\n","\n","    # List of indices\n","    source_text_to_indices = [source_field.vocab.stoi[token] for token in input_tokens]\n","    source_tensor = torch.LongTensor(source_text_to_indices).unsqueeze(0).to(device)\n","   \n","    outputs = [targer_field.vocab.stoi[\"<sos>\"]]\n","    for i in range(max_length):\n","        target_tensor = torch.LongTensor(outputs).unsqueeze(0).to(device)\n","\n","        with torch.no_grad():\n","            output = model(source_tensor, target_tensor)\n","\n","        best_guess = output[0].argmax(1)[-1].item()\n","        outputs.append(best_guess)\n","\n","        if best_guess == targer_field.vocab.stoi[\"<eos>\"]:\n","            break\n","\n","    translated_sentence = [targer_field.vocab.itos[idx] for idx in outputs]\n","    # remove start token\n","    return translated_sentence[1:]\n","\n","\n","def epoch_time(start_time, end_time):\n","    elapsed_time = end_time - start_time\n","    elapsed_mins = int(elapsed_time / 60)\n","    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n","    return elapsed_mins, elapsed_secs\n","\n","def bleu(data, model, spacy_german, german_field, english_field, device):\n","\n","    targets = []\n","    outputs = []\n","\n","\n","    for example in data:\n","        src = vars(example)[\"src\"]\n","        trg = vars(example)[\"trg\"]\n"," \n","        predict = translate_sentence(model, src, spacy_german, german_field, english_field, device)\n","        predict = predict[:-1]\n","\n","        targets.append([trg])\n","        outputs.append(predict)\n","\n","    return bleu_score(outputs, targets)\n"]},{"cell_type":"code","execution_count":2,"metadata":{"executionInfo":{"elapsed":883,"status":"ok","timestamp":1643769599652,"user":{"displayName":"Mir Shahriar Sabuj","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgnIF9MHarEcGwKaxaBrV3ZiuS6e9hwUsKhsKJ_=s64","userId":"07212414285941174156"},"user_tz":-360},"id":"_mfbrQLhUNJM"},"outputs":[],"source":["from turtle import forward\n","from unicodedata import bidirectional\n","import torch\n","import torch.nn as nn\n","from torch.autograd import Variable\n","import numpy as np\n","\n","class Embedder(nn.Module):\n","    def __init__(self, input_size, d_model):\n","        super().__init__()\n","        self.embedding = nn.Embedding(input_size, d_model)\n","    def forward(self, x):\n","        return self.embedding(x)\n","\n","\n","class PositionalEncoding(nn.Module):\n","    def __init__(self, d_model, max_length = 100):\n","        super().__init__()\n","        self.d_model = d_model\n","        positional_data = torch.zeros(max_length, d_model)\n","\n","        def get_positional_value(pos):\n","            return [pos / np.power(10000, 2 * (i // 2) / d_model) for i in range(d_model)]\n","\n","        positional_data = np.array([get_positional_value(pos) for pos in range(max_length)])\n","        positional_data[:, 0::2] = np.sin(positional_data[:, 0::2])  # dim 2i\n","        positional_data[:, 1::2] = np.cos(positional_data[:, 1::2])  # dim 2i+1\n","\n","        positional_data = torch.Tensor(positional_data).unsqueeze(0)\n","        self.register_buffer('pe', positional_data)\n"," \n","    def forward(self, x):\n","        seq_length = x.size(1)\n","        x = x + Variable(self.pe[:,:seq_length], requires_grad=False)\n","        return x\n","\n","\n","class MultiHeadAttention(nn.Module):\n","    def __init__(self, d_model, n_heads):\n","        super(MultiHeadAttention, self).__init__()\n","        self.n_heads = n_heads\n","        # Divide vector into equal n_heads part\n","        d_h = d_model // n_heads\n","        self.d_h = d_h\n","\n","        # Following linear models produce Query, Key & Value for words\n","        # d_model -> d_h * n_heads\n","        self.WQ = nn.Linear(d_model, d_model)\n","        self.WK = nn.Linear(d_model, d_model)\n","        self.WV = nn.Linear(d_model, d_model)\n","\n","        self.softmax = nn.Softmax(dim=-1)\n","\n","        self.linear = nn.Linear(d_model, d_model)\n","\n","\n","    def forward(self, Q, K, V, masked=None):\n","        # Q: (batch_size, seq_length, d_model) -- d_model == embedding_dim\n","        # K: (batch_size, seq_length, d_model)\n","        # V: (batch_size, seq_length, d_model)\n","\n","        batch_size = Q.size(0)\n"," \n","        Q = self.WQ(Q).view(batch_size, -1, self.n_heads, self.d_h).transpose(1, 2)\n","        # Q: (batch_size, seq_length, n_heads, d_h) -> (batch_size, n_heads, seq_length, d_h)\n","        K = self.WK(K).view(batch_size, -1, self.n_heads, self.d_h).transpose(1, 2)\n","        # K: (batch_size, seq_length, n_heads, d_h) -> (batch_size, n_heads, seq_length, d_h)\n","        V = self.WV(V).view(batch_size, -1, self.n_heads, self.d_h).transpose(1, 2)\n","        # V: (batch_size, seq_length, n_heads, d_h) -> (batch_size, n_heads, seq_length, d_h)\n","\n","        # Q: (batch_size, n_heads, seq_length, d_h)\n","        # K: (batch_size, n_heads, seq_length, d_h) -> (batch_size, n_heads, d_h, seq_length)\n","        # Formula: (a,b,c,d)*(a,b,d,f) -> (a,b,c,f)\n","        scores = torch.matmul(Q, K.transpose(-1, -2)) / np.sqrt(self.d_h)\n","        # scores: (batch_size, n_heads, seq_length, seq_length)\n","        # This is actually calculating word-by-word score. Thats why shape is (-, -, seq_length, seq_length)\n","\n","\n","        if masked is not None:\n","            # pad_masked: (batch_size, seq_length, seq_length)\n","            masked = masked.unsqueeze(1).repeat(1, self.n_heads, 1, 1)\n","            # pad_masked: (batch_size, n_heads, seq_length, seq_length)\n","            # The shape of the mask is exactly same as scores.\n","            scores = scores.masked_fill(masked, -1e9)\n","            # scores: (batch_size, n_heads, seq_length, seq_length)\n","\n","        attention = self.softmax(scores)\n","\n","        # attention: (batch_size, n_heads, seq_length, seq_length)\n","        # V: (batch_size, n_heads, seq_length, d_h)\n","        context = torch.matmul(attention, V).transpose(1, 2).contiguous()\n","        # context: (batch_size, n_heads, seq_length, d_h) -> (batch_size, seq_length, n_heads, d_h)\n","\n","        # The following part concat several heads into one\n","        output = context.view(batch_size, -1, self.n_heads * self.d_h)\n","        # output: (batch_size, seq_length, d_model)\n","        output = self.linear(output)\n","        # output: (batch_size, seq_length, d_model)\n","\n","        return output\n","\n","\n","class PoswiseFeedForwardNet(nn.Module):\n","\n","    def __init__(self, d_model, ffn_dim,):\n","        super(PoswiseFeedForwardNet, self).__init__()\n","        self.l1 = nn.Linear(d_model, ffn_dim)\n","        self.l2 = nn.Linear(ffn_dim, d_model)\n","\n","    def forward(self, x):\n","        output = self.l1(x)\n","        output = torch.relu(output)\n","        output = self.l2(output)\n","        return output\n","\n","\n","# Layer of Encoder\n","class EncoderLayer(nn.Module):\n","    def __init__(self, d_model, n_heads, ffn_dim, dropout=0.1):\n","        super(EncoderLayer, self).__init__()\n","\n","        # The first sub-layer of Encoder. Its a multi-head self-attention.\n","        self.self_attention = MultiHeadAttention(d_model, n_heads)\n","        self.self_attention_norm = nn.LayerNorm(d_model)\n","        # The second sub-layer of Encoder. Its a positionwise fully connected feed-forward network.\n","        self.ff_layer = PoswiseFeedForwardNet(d_model, ffn_dim)\n","        self.ff_layer_norm = nn.LayerNorm(d_model)\n","\n","        self.dropout = nn.Dropout(dropout)\n","\n","    def forward(self, source, pad_masked):\n","        # As we are feeding same input as query, key & value, its called self-attention.\n","        # source: (batch_size, seq_length, d_model)\n","        # pad_masked: (batch_size, seq_length, seq_length)\n","        output_from_attn = self.self_attention(source, source, source, pad_masked)\n","        output_from_attn = self.dropout(output_from_attn)\n","        output_from_attn = self.self_attention_norm(source + output_from_attn)\n","        # output_from_attn: (batch_size, seq_length, d_model)\n","        output_from_ffn = self.ff_layer(output_from_attn)\n","        output_from_ffn = self.dropout(output_from_ffn)\n","        output_from_ffn = self.ff_layer_norm(output_from_attn + output_from_ffn)\n","        # output_from_ffn: (batch_size, seq_length, d_model)\n","\n","        return output_from_ffn\n","\n","\n","# Encoder is responsible to represent\n","# a source sentence into a context state\n","class Encoder(nn.Module):\n","    def __init__(self, source_vocab_size, d_model, n_layers, n_heads, ffn_dim, source_pad_idx, dropout=0.1) :\n","        super(Encoder, self).__init__()\n","\n","        self.source_pad_idx = source_pad_idx\n","\n","        # It's like Word2Vec\n","        self.embedding = nn.Embedding(source_vocab_size, d_model)\n","        # This is used to add position data with embedded words.\n","        self.position_embedding = PositionalEncoding(d_model)\n","\n","        self.dropout = nn.Dropout(dropout)\n","\n","        # Series of similar layers.\n","        self.layers = nn.ModuleList([EncoderLayer(d_model, n_heads, ffn_dim) for _ in range(n_layers)])\n","\n","    def forward(self, source):\n","        # This method will add masking to padded position (<pad>).\n","        # source: (batch_size, seq_length)\n","        pad_masked = get_padding_mask(source, source, self.source_pad_idx)\n","        # pad_masked: (batch_size, seq_length, seq_length)\n","\n","        embedded = self.embedding(source)\n","        # embedded: (batch_size, seq_length, embedding_dim)\n","        embedded = self.position_embedding(embedded)\n","        # embedded: (batch_size, seq_length, embedding_dim)\n","        embedded = self.dropout(embedded)\n","\n","        # The first input to the series of layer comes from embedded input.\n","        # After that, output of one layer is fed into the next layer.\n","        for layer in self.layers:\n","            embedded = layer(embedded, pad_masked)\n","\n","        return embedded\n","\n","\n","class DecoderLayer(nn.Module):\n","    def __init__(self, d_model, n_heads, ffn_dim, dropout=0.1):\n","        super(DecoderLayer, self).__init__()\n","        \n","        # The first sub-layer of Decoder. Its a multi-head self-attention.\n","        self.self_attention = MultiHeadAttention(d_model, n_heads)\n","        self.self_attention_norm = nn.LayerNorm(d_model)\n","        # The second sub-layer of Decoder. Its an encoder-decoder multi-head self-attention.\n","        self.codec_attention= MultiHeadAttention(d_model, n_heads)\n","        self.codec_attention_norm = nn.LayerNorm(d_model)\n","        # The third sub-layer of Encoder. Its a positionwise fully connected feed-forward network.\n","        self.ffn_layer = PoswiseFeedForwardNet(d_model, ffn_dim)\n","        self.ffn_layer_norm = nn.LayerNorm(d_model)\n","        \n","        self.dropout = nn.Dropout(dropout)\n","\n","    def forward(self, target, source, pad_masked):\n","        # As we are feeding same input as query, key & value, its called self-attention.\n","        # target: (batch_size, seq_length, d_model)\n","        # pad_masked: (batch_size, seq_length, seq_length)\n","        output_from_attn = self.self_attention(target, target, target, pad_masked)\n","        output_from_attn = self.dropout(output_from_attn)\n","        output_from_attn = self.self_attention_norm(target + output_from_attn)\n","        # output_from_attn\n","        # output_from_attn : (batch_size, seq_length, d_model)\n","        # source : (batch_size, seq_length, d_model)\n","        output_from_codec_attn= self.codec_attention(output_from_attn, source, source, None)\n","        output_from_codec_attn = self.dropout(output_from_codec_attn)\n","        output_from_codec_attn = self.codec_attention_norm(output_from_attn + output_from_codec_attn)\n","        # output_from_codec_attn\n","        output_ffn = self.ffn_layer(output_from_codec_attn)\n","        output_ffn = self.dropout(output_ffn)\n","        output_ffn = self.ffn_layer_norm(output_from_codec_attn + output_ffn)\n","        # output_ffn\n","\n","        return output_ffn\n","\n","# Decoder is responsible to generate\n","# a target sentence from a context state\n","class Decoder(nn.Module):\n","    def __init__(self, target_vocab_size, d_model, n_layers, n_heads, ffn_dim, source_pad_idx, target_pad_idx, device, dropout=0.1):\n","        super(Decoder, self).__init__()\n","\n","        self.source_pad_idx = source_pad_idx\n","        self.target_pad_idx = target_pad_idx\n","        self.device = device\n","        \n","        # It's like Word2Vec\n","        self.embedding = nn.Embedding(target_vocab_size, d_model)\n","        # This is used to add position data with embedded words.\n","        self.position_embedding = PositionalEncoding(d_model)\n","\n","        self.dropout = nn.Dropout(dropout)\n","\n","        # Series of similar layers.\n","        self.layers = nn.ModuleList([DecoderLayer(d_model, n_heads, ffn_dim) for _ in range(n_layers)])\n","        # This is to project output\n","        self.fc_out = nn.Linear(d_model, target_vocab_size)\n","\n","    def forward(self, target, source_output):\n","        # This method will add masking to padded position (<pad>).\n","        # target: (batch_size, seq_length)\n","        target_mask = get_padding_mask(target, target, self.target_pad_idx)\n","        # This method will add masking to subsequent position.\n","        subsequent_mask = get_subsequent_mask(target, self.device)\n","        target_mask = target_mask | subsequent_mask\n","\n","        embedded = self.embedding(target)\n","        # embedded: (batch_size, seq_length, embedding_dim)\n","        embedded = self.position_embedding(embedded)\n","        # embedded: (batch_size, seq_length, embedding_dim)\n","        embedded = self.dropout(embedded)\n","\n","        # The first input to the series of layer comes from embedded input.\n","        # After that, output of one layer is fed into the next layer. \n","        for layer in self.layers:\n","            embedded = layer(embedded, source_output, target_mask)\n","\n","        # This is to project output\n","        output = self.fc_out(embedded)\n","\n","        return output\n","\n","\n","# Seq2Seq combines Encoder & Decoder\n","class Seq2Seq(nn.Module):\n","    def __init__(self, encoder, decoder):\n","        super(Seq2Seq, self).__init__()\n","\n","        self.encoder = encoder\n","        self.decoder = decoder\n","\n","    def forward(self, source, target):\n","        # source: (batch_size, seq_length)\n","        # target: (batch_size, seq_length)\n","        encoded = self.encoder(source)\n","        # encoded: (batch_size, seq_length, hidden_dim)\n","        output = self.decoder(target, encoded)\n","        # output: (batch_size, seq_length, target_vocab_size)\n","\n","        return output\n"]},{"cell_type":"code","execution_count":3,"metadata":{"executionInfo":{"elapsed":2,"status":"ok","timestamp":1643769599653,"user":{"displayName":"Mir Shahriar Sabuj","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgnIF9MHarEcGwKaxaBrV3ZiuS6e9hwUsKhsKJ_=s64","userId":"07212414285941174156"},"user_tz":-360},"id":"vblm3brqUYtb"},"outputs":[],"source":["import torch\n","import time\n","import math\n","\n","\n","class Process():\n","    def __init__(self, model, src_spacy_model, source_field, target_field, optimizer, scheduler, loss_func, test_sentence, clip, device):\n","        self.model = model\n","        self.src_spacy_model = src_spacy_model\n","        self.source_field = source_field\n","        self.target_field = target_field\n","        self.optimizer = optimizer\n","        self.scheduler = scheduler\n","        self.loss_func = loss_func\n","        self.test_sentence = test_sentence\n","        self.clip = clip\n","        self.device = device\n","        self.step = 0\n","\n","    # Train model\n","    def train(self, iterator):\n","\n","        epoch_loss = 0\n","\n","        self.model.train()\n","\n","        for batch in iterator:\n","            source = batch.src.to(self.device)\n","            target = batch.trg.to(self.device)\n","            # source: (batch_size, seq_length)\n","            # target: (batch_size, seq_length)\n","\n","            # Last target token is <eos>. Do no need to pass it.\n","            output = self.model(source, target[:,:-1])\n","            # output: (batch_size, seq_length, target_vocab_size)\n"," \n","            target_vocab_size = output.shape[-1]\n","            output = output.contiguous().view(-1, target_vocab_size)\n","            # output: (batch_size * seq_length, target_vocab_size)\n","\n","            target = target[:,1:].contiguous().view(-1)\n","            # target: (batch_size, seq_length)\n","\n","            self.optimizer.zero_grad()\n","            loss = self.loss_func(output, target)\n","            loss.backward()\n","\n","            # This is used to prevent gradient exploding. Clipping to 1.\n","            torch.nn.utils.clip_grad_norm_(self.model.parameters(), self.clip)\n","\n","            # This updates the trainable parameters\n","            self.optimizer.step()\n","\n","            self.step += 1\n","\n","            epoch_loss += loss.item()\n","            \n","            \n","        return epoch_loss / len(iterator)\n","\n","    def evaluate(self, iterator):\n","\n","        self.model.eval()\n","\n","        epoch_loss = 0\n","\n","        with torch.no_grad():\n","\n","            for batch in iterator:\n","\n","                source = batch.src.to(self.device)\n","                target = batch.trg.to(self.device)\n","\n","                output = self.model(source, target[:,:-1])\n","                \n","                target_vocab_size = output.shape[-1]\n","\n","                output = output.contiguous().view(-1, target_vocab_size)\n","                target = target[:,1:].contiguous().view(-1)\n","\n","                loss = self.loss_func(output, target)\n","\n","                epoch_loss += loss.item()\n","\n","\n","        return epoch_loss / len(iterator)\n","\n","    def run(self, num_epochs, train_iterator, valid_iterator):\n","\n","        best_lost = 1e10\n","        no_better = 0\n","\n","        for epoch in range(num_epochs):\n","\n","            start_time = time.time()\n","            \n","            train_loss = self.train(train_iterator)\n","            valid_loss = self.evaluate(valid_iterator)\n","\n","            end_time = time.time()\n","            epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n","            lr = self.scheduler.get_last_lr()[0]\n","\n","            print(f'Epoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs}s')\n","            print(f'\\tTrain Loss: {train_loss:.3f} | Train Exp: {math.exp(train_loss):7.3f}')\n","            print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. Exp: {math.exp(valid_loss):7.3f}')\n","            print(f'\\t Learning Rate: {lr:.7f}')\n","\n","            # This translate the sample sentence.\n","            translated_sentence = translate_sentence(\n","                self.model, self.test_sentence,\n","                self.src_spacy_model, self.source_field, self.target_field,\n","                self.device, max_length=50,\n","            )\n","\n","            print(f\"Translated example sentence: \\n {' '.join(translated_sentence[:-1])}\")\n","            print()\n","            self.scheduler.step()\n","\n","            if valid_loss > best_lost:\n","              no_better += 1\n","              if no_better == 3:\n","                break\n","            else: \n","              best_lost = valid_loss\n","              no_better=0\n"]},{"cell_type":"code","execution_count":4,"metadata":{"executionInfo":{"elapsed":2,"status":"ok","timestamp":1643769599653,"user":{"displayName":"Mir Shahriar Sabuj","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgnIF9MHarEcGwKaxaBrV3ZiuS6e9hwUsKhsKJ_=s64","userId":"07212414285941174156"},"user_tz":-360},"id":"Rp5kPtlLUfSH"},"outputs":[],"source":["# !pip install spacy==v3.2\n","# !python -m spacy download de_core_news_lg\n","# !python -m spacy download en_core_web_lg"]},{"cell_type":"code","execution_count":5,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"kLDb9q77UeB0","executionInfo":{"status":"ok","timestamp":1643769886771,"user_tz":-360,"elapsed":287120,"user":{"displayName":"Mir Shahriar Sabuj","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgnIF9MHarEcGwKaxaBrV3ZiuS6e9hwUsKhsKJ_=s64","userId":"07212414285941174156"}},"outputId":"461c3b31-2571-4379-91a1-09b373218c71"},"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch: 01 | Time: 0m 15s\n","\tTrain Loss: 3.831 | Train Exp:  46.098\n","\t Val. Loss: 2.854 |  Val. Exp:  17.349\n","\t Learning Rate: 0.0005000\n","Translated example sentence: \n"," the old woman is looking at the park and holding her face .\n","\n","Epoch: 02 | Time: 0m 15s\n","\tTrain Loss: 2.561 | Train Exp:  12.947\n","\t Val. Loss: 2.257 |  Val. Exp:   9.553\n","\t Learning Rate: 0.0004750\n","Translated example sentence: \n"," the old woman is looking at the soccer game .\n","\n","Epoch: 03 | Time: 0m 15s\n","\tTrain Loss: 2.085 | Train Exp:   8.047\n","\t Val. Loss: 2.009 |  Val. Exp:   7.458\n","\t Learning Rate: 0.0004512\n","Translated example sentence: \n"," the old woman is looking at the soccer game .\n","\n","Epoch: 04 | Time: 0m 15s\n","\tTrain Loss: 1.786 | Train Exp:   5.965\n","\t Val. Loss: 1.860 |  Val. Exp:   6.425\n","\t Learning Rate: 0.0004287\n","Translated example sentence: \n"," the old woman is looking at the soccer game .\n","\n","Epoch: 05 | Time: 0m 15s\n","\tTrain Loss: 1.562 | Train Exp:   4.768\n","\t Val. Loss: 1.779 |  Val. Exp:   5.925\n","\t Learning Rate: 0.0004073\n","Translated example sentence: \n"," the old woman is looking at the soccer ball and eating .\n","\n","Epoch: 06 | Time: 0m 15s\n","\tTrain Loss: 1.381 | Train Exp:   3.978\n","\t Val. Loss: 1.730 |  Val. Exp:   5.641\n","\t Learning Rate: 0.0003869\n","Translated example sentence: \n"," the old woman is looking at the soccer ball and eating .\n","\n","Epoch: 07 | Time: 0m 15s\n","\tTrain Loss: 1.229 | Train Exp:   3.416\n","\t Val. Loss: 1.721 |  Val. Exp:   5.592\n","\t Learning Rate: 0.0003675\n","Translated example sentence: \n"," the old woman is looking at the soccer ball and eating .\n","\n","Epoch: 08 | Time: 0m 15s\n","\tTrain Loss: 1.095 | Train Exp:   2.989\n","\t Val. Loss: 1.725 |  Val. Exp:   5.612\n","\t Learning Rate: 0.0003492\n","Translated example sentence: \n"," the old woman is at the soccer game , as she is eating .\n","\n","Epoch: 09 | Time: 0m 15s\n","\tTrain Loss: 0.977 | Train Exp:   2.658\n","\t Val. Loss: 1.717 |  Val. Exp:   5.569\n","\t Learning Rate: 0.0003317\n","Translated example sentence: \n"," the old woman is looking at the soccer game .\n","\n","Epoch: 10 | Time: 0m 15s\n","\tTrain Loss: 0.873 | Train Exp:   2.393\n","\t Val. Loss: 1.741 |  Val. Exp:   5.701\n","\t Learning Rate: 0.0003151\n","Translated example sentence: \n"," the old woman is looking at the soccer ball and eating candy .\n","\n","Epoch: 11 | Time: 0m 15s\n","\tTrain Loss: 0.780 | Train Exp:   2.182\n","\t Val. Loss: 1.773 |  Val. Exp:   5.890\n","\t Learning Rate: 0.0002994\n","Translated example sentence: \n"," the old woman is watching the soccer game while eating candy .\n","\n","Epoch: 12 | Time: 0m 15s\n","\tTrain Loss: 0.697 | Train Exp:   2.008\n","\t Val. Loss: 1.817 |  Val. Exp:   6.154\n","\t Learning Rate: 0.0002844\n","Translated example sentence: \n"," the old woman is watching the soccer game while eating candy .\n","\n","Bleu score 36.29\n"]}],"source":["import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from torchtext.legacy.datasets import Multi30k\n","from torchtext.legacy.data import Field, BucketIterator, TabularDataset\n","import numpy as np\n","import spacy\n","import random\n","\n","\n","SEED = 1876189809\n","random.seed(SEED)\n","np.random.seed(SEED)\n","torch.manual_seed(SEED)\n","torch.cuda.manual_seed(SEED)\n","torch.backends.cudnn.deterministic = True\n","\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","# I have used spacy dataset for low volume\n","# Sentences:\n","#   Training:   29000\n","#   Validating: 1014\n","#   Testing:    1000\n","# Vocabulary:\n","#   German:     7853\n","#   English:    5893\n","spacy_german = spacy.load(\"de_core_news_lg\")\n","spacy_english = spacy.load(\"en_core_web_lg\")\n","\n","def tokenize_ger(text):\n","    return [tok.text for tok in spacy_german.tokenizer(text)]\n","\n","def tokenize_eng(text):\n","    return [tok.text for tok in spacy_english.tokenizer(text)]\n","\n","\n","# This is used to tokenize and append extra token\n","german_field = Field(tokenize=tokenize_ger, lower=True, init_token=\"<sos>\", eos_token=\"<eos>\", batch_first=True)\n","english_field = Field(tokenize=tokenize_eng, lower=True, init_token=\"<sos>\", eos_token=\"<eos>\", batch_first=True)\n","\n","\n","# To load data from local file\n","# fields = {\"src\": (\"src\", german_field), \"trg\": (\"trg\", english_field)}\n","# train_data, valid_data, test_data = TabularDataset.splits(\n","#     path=\"/content\", train=\"train.txt\", validation=\"val.txt\", test=\"test.txt\", format=\"json\", fields=fields\n","# )\n","\n","\n","train_data, valid_data, test_data = Multi30k.splits(\n","    exts=(\".de\", \".en\"),\n","    fields=(german_field, english_field)\n",")\n","\n","\n","# This will generate vocabulary with minimum freq.\n","german_field.build_vocab(train_data, min_freq=2)\n","english_field.build_vocab(train_data, min_freq=2)\n","source_vocab_size = len(german_field.vocab)\n","target_vocab_size = len(english_field.vocab)\n","\n","num_epochs = 1000\n","learning_rate = 0.0005\n","# Large batch size exceeds free GPU memory\n","batch_size = 100\n","# Dimension for all matrix\n","# I will call it hidden_dim & embedding_dim as well\n","d_model = 300\n","# Number of repeated layers\n","n_layers = 4\n","# Number of heads\n","n_heads = 4\n","ffn_dim = 1200\n","\n","# Use this to clip gradient norm to avoid exploding\n","clip = 1\n","dropout = 0.1\n","\n","source_pad_idx = german_field.vocab.stoi[german_field.pad_token]\n","target_pad_idx = english_field.vocab.stoi[english_field.pad_token]\n","\n","encoder = Encoder(source_vocab_size, d_model, n_layers, n_heads, ffn_dim, source_pad_idx, dropout=dropout).to(device)\n","decoder = Decoder(target_vocab_size, d_model, n_layers, n_heads, ffn_dim, source_pad_idx, target_pad_idx, device, dropout=dropout).to(device)\n","# german_vector = torch.FloatTensor(spacy_german.vocab.vectors.data)\n","# encoder.embedding = nn.Embedding.from_pretrained(german_vector, freeze=False)\n","# english_vector = torch.FloatTensor(spacy_english.vocab.vectors.data)\n","# decoder.embedding = nn.Embedding.from_pretrained(english_vector, freeze=False)\n","\n","\n","model = Seq2Seq(encoder, decoder).to(device)\n","\n","# This is to use Adam optimizer\n","optimizer = optim.Adam(model.parameters(), lr=learning_rate, betas=(0.9, 0.98), eps=1e-9)\n","scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1.0, gamma=0.95)\n","\n","\n","# This is to use CrossEntropy loss function.\n","# Ignore padding entry\n","loss_func = nn.CrossEntropyLoss(ignore_index=target_pad_idx)\n","\n","train_iterator, validation_iterator, test_iterator = BucketIterator.splits(\n","    (train_data, valid_data, test_data),\n","    batch_size=batch_size,\n","    sort_key = lambda x: len(x.src),\n","    sort_within_batch=True,\n","    device=device,\n",")\n","\n","\n","test_sentence = \"Die alte Frau sieht sich das Fußballspiel an und isst Süßigkeiten.\"\n","test_translation = \"The old woman is watching the football match and eating candy.\"\n","\n","process = Process(model, spacy_german, german_field, english_field, optimizer, scheduler, loss_func, test_sentence, clip, device)\n","\n","process.run(num_epochs, train_iterator, validation_iterator)\n","\n","score = bleu(test_data, model, spacy_german, german_field, english_field, device)\n","print(f\"Bleu score {score*100:.2f}\")"]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":[],"machine_shape":"hm","name":"only_attention.ipynb","provenance":[],"mount_file_id":"1t8chxcHP9Q7oW5BPRiFDzDI9BLn86851","authorship_tag":"ABX9TyP/3gxsTulfqk1Yu/3ZCzon"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}